{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CS 181 HW5**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize data and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a specific example of when we have $K = 3$ component Gamma distributions. Let's initialize the initial parameter values for $\\theta$ and $\\beta_k$ as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\theta_k &=  1/K, \\\\\n",
    "  \\beta_k & = k/K.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that we usually initialize $\\theta$ and $\\beta_k$ randomly. However, by fixing the initial $\\theta$ and $\\beta_k$, EM becomes deterministic which makes debugging (and grading) easier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as ds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "x = torch.load('data.pt')\n",
    "if isinstance(x, np.ndarray):\n",
    "    x = torch.from_numpy(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to use numpy (optional)\n",
    "# import numpy as np\n",
    "# from scipy.stats import gamma\n",
    "# x = x.numpy()\n",
    "# theta = theta.numpy()\n",
    "# betas = betas.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement the E-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(theta, betas):\n",
    "    q = ds.Gamma(alpha, betas).log_prob(x) + theta.log() \n",
    "    q = q - q.logsumexp(dim=1, keepdim=True)\n",
    "    q = q.exp() \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement the M-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(q):\n",
    "    theta_hat = q.mean(dim=0)\n",
    "    betas_hat = (alpha * q.sum(dim=0)) / (q * x.unsqueeze(-1)).sum(dim=0)\n",
    "    return theta_hat, betas_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_px(x, theta, betas):\n",
    "    p = torch.zeros(x.shape[0], len(betas))\n",
    "    for k in range(len(betas)):\n",
    "        gamma_dist = ds.Gamma(alpha, betas[k])\n",
    "        p[:, k] = theta[k] * gamma_dist.log_prob(x).exp()\n",
    "    return torch.log(p.sum(dim=1))\n",
    "\n",
    "def log_likelihood(theta, betas):\n",
    "    return log_px(x, theta, betas).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_em(theta, betas, iterations=1000):\n",
    "    for _ in range(iterations):\n",
    "        q = e_step(theta, betas)\n",
    "        theta, betas = m_step(q)\n",
    "    return theta, betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overlay_plot(theta, betas):\n",
    "    x_test = torch.linspace(0.01, x.max(), 1000).unsqueeze(1)  # Make x_test two-dimensional\n",
    "    prob = log_px(x_test, theta, betas).exp()\n",
    "    # prob = np.exp(log_px(x_test.unsqueeze(-1), theta, betas))  # use this line for numpy\n",
    "    ll = log_likelihood(theta, betas)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5, 3))\n",
    "    fig.subplots_adjust(top=0.7)\n",
    "    fig.suptitle(f'theta = {theta}\\nbeta = {betas}\\nlog likelihood = {ll:.3e}')\n",
    "    \n",
    "    ax.hist(x.T, bins=100, color='tomato', alpha=0.5, density=True, label='Dataset')\n",
    "    ax.plot(x_test, prob, color='royalblue', label='Gamma mixture')\n",
    "    \n",
    "    ax.set_title(f'Dataset and Gamma mixture (K={len(theta)})')\n",
    "    ax.set_xlabel('Recovery time (hours)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[1000, 1]}, size=[1000]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z_/z80nfr_11pb4k54vwnh48y_h0000gp/T/ipykernel_74238/3307709265.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_em\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmake_overlay_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'p2_3_{K}mixtures.pdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/z_/z80nfr_11pb4k54vwnh48y_h0000gp/T/ipykernel_74238/3001269819.py\u001b[0m in \u001b[0;36mmake_overlay_plot\u001b[0;34m(theta, betas)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_overlay_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make x_test two-dimensional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_px\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# prob = np.exp(log_px(x_test.unsqueeze(-1), theta, betas))  # use this line for numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/z_/z80nfr_11pb4k54vwnh48y_h0000gp/T/ipykernel_74238/2795996807.py\u001b[0m in \u001b[0;36mlog_px\u001b[0;34m(x, theta, betas)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mgamma_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[1000, 1]}, size=[1000]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "alpha = 5.0\n",
    "for K in range(1,5):\n",
    "    theta0 = torch.ones(K) / K\n",
    "    betas0 = (torch.arange(K) + 1) / K\n",
    "    # theta0 = np.ones(K) / K               # use this for numpy\n",
    "    # betas0 = (np.arange(K) + 1) / K\n",
    "    \n",
    "    theta, betas = run_em(theta0, betas0)\n",
    "    \n",
    "    make_overlay_plot(theta, betas)\n",
    "    plt.savefig(f'p2_3_{K}mixtures.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 13007689.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 92329034.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 11674282.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 17955258.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)  # download MNIST\n",
    "N = 6000 \n",
    "\n",
    "x = mnist_trainset.data[:N]  # select N datapoints\n",
    "x = x.flatten(1)             # flatten the images\n",
    "x = x.float()                # convert pixels from uint8 to float\n",
    "# x = x.numpy()              # uncomment to use numpy (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** implement PCA\n",
    "\n",
    "*Hint: see `.linalg.svd()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x, n_comps=500):\n",
    "    x_centered = x - x.mean(dim=0)\n",
    "    cov_matrix = torch.matmul(x_centered.T, x_centered) / (x.size(0) - 1)\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(cov_matrix, UPLO='U')\n",
    "    top_eigvals = eigenvalues[-n_comps:].flip(dims=(0,))\n",
    "    top_pcomps = eigenvectors[:, -n_comps:].flip(dims=(1,))\n",
    "    return top_eigvals, top_pcomps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** calculate cumulative fraction of variance\n",
    "\n",
    "*Hint: see `.cumsum()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cfvs(eigvals):\n",
    "    total_variance = torch.sum(eigvals)\n",
    "    cfvs = torch.cumsum(eigvals, dim=0) / total_variance\n",
    "    return cfvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Todo:** calculate mean squared L2 norm reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_errs(x, pcomps):\n",
    "    x_centered = x - x.mean(dim=0)\n",
    "    x_projected = torch.matmul(x_centered, pcomps)\n",
    "    reconstruction = torch.matmul(x_projected, pcomps.T) + x.mean(dim=0)\n",
    "    err_mean = (x_centered ** 2).mean()\n",
    "    err_pcomp = ((x - reconstruction) ** 2).mean()\n",
    "    return err_mean, err_pcomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and print errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pic(pic, ax, title=''):\n",
    "    x = pic.reshape(28, 28)\n",
    "    ax.imshow(x, cmap='binary')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "def make_plots(eigvals, cfvs, x_mean, pcomps):\n",
    "    # plot eigenvals and cfvs\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    ax1.plot(eigvals, color='tomato')\n",
    "    ax1.set_title('Eigenvalues')\n",
    "    ax2.plot(cfvs, color='tomato')\n",
    "    ax2.set_title('CFVs')\n",
    "    fig.savefig('p3_cfvs.pdf')\n",
    "\n",
    "    # plot mean\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "    plot_pic(x_mean, ax, title='Mean')\n",
    "    fig.savefig('p3_mean.pdf')\n",
    "\n",
    "    # plot top 10 pcomps\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    for i in range(10):\n",
    "        plot_pic(pcomps[i], axes.flat[i], title=f'PC index {i}')\n",
    "    fig.savefig('p3_pcomps.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got 6000, 6000x784,500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z_/z80nfr_11pb4k54vwnh48y_h0000gp/T/ipykernel_74238/400988723.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0merr_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_pcomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_errs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpcomps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Reconstruction error (using mean): {err_mean:3e}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Reconstruction error (using mean and top 10 pcomps): {err_pcomp:3e}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/z_/z80nfr_11pb4k54vwnh48y_h0000gp/T/ipykernel_74238/3550975295.py\u001b[0m in \u001b[0;36mcalc_errs\u001b[0;34m(x, pcomps)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_errs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpcomps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx_centered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx_projected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_centered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpcomps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_projected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpcomps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0merr_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_centered\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, got 6000, 6000x784,500"
     ]
    }
   ],
   "source": [
    "# do PCA\n",
    "eigvals, pcomps = pca(x)\n",
    "\n",
    "# calculate CFVs\n",
    "fcvs = calc_cfvs(eigvals)\n",
    "\n",
    "# print errors\n",
    "err_mean, err_pcomp = calc_errs(x, pcomps)\n",
    "print(f'Reconstruction error (using mean): {err_mean:3e}')\n",
    "print(f'Reconstruction error (using mean and top 10 pcomps): {err_pcomp:3e}')\n",
    "\n",
    "# make plots\n",
    "make_plots(eigvals, fcvs, x.mean(0), pcomps)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c365ea26a318e0b540d1978e97e3d03cfe51dff8cd04dae5f3d7b47d79d2453"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
